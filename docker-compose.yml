services:
  # ----------------------------
  # 1) MinIO = "S3-compatible" object storage
  # ----------------------------
  # We use MinIO as a local replacement for AWS S3.
  # Spark will read/write data to MinIO using the s3a:// protocol.
  minio:
    image: minio/minio:latest
    # Run MinIO server and also expose the web console on port 9001
    command: server /data --console-address ":9001"

    # Environment variables for MinIO admin credentials.
    # These are loaded from your local .env file.
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}

    # A named volume stores data on your machine even if the container restarts.
    # This is where MinIO keeps your buckets and files.
    volumes:
      - minio:/data

    # Expose container ports to your host (your laptop):
    # - 9000 = MinIO API (S3 endpoint)
    # - 9001 = MinIO Web Console (UI)
    ports:
      - "9000:9000"
      - "9001:9001"

  # ----------------------------
  # 2) createbuckets = one-time setup container
  # ----------------------------
  # This container uses the MinIO Client ("mc") to create the bucket we need.
  # It runs a small shell script and then exits.
  createbuckets:
    image: minio/mc:latest

    # Wait until MinIO service is started before running.
    depends_on:
      - minio

    # entrypoint overrides the default command and runs our script.
    # What the script does:
    # 1) Configure an alias called "local" pointing to MinIO endpoint
    # 2) Create a bucket (LAKEHOUSE_BUCKET) if it doesn't exist
    #
    # Notes:
    # - "|| true" prevents the container from failing if the bucket already exists
    entrypoint: >
      /bin/sh -c "
      echo 'Waiting for MinIO to be ready...';
      until mc alias set local ${MINIO_ENDPOINT} ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD}; do
        echo 'MinIO not ready yet - sleeping 2s';
        sleep 2;
      done;
      echo 'MinIO is ready. Creating bucket...';
      mc mb -p local/${LAKEHOUSE_BUCKET} || true;
      echo 'Done.';
      "


  # ----------------------------
  # 3) Spark Master
  # ----------------------------
  # Spark runs as a cluster:
  # - 1 master node manages the cluster
  # - 1+ worker nodes execute the jobs
  #
  # We use Bitnami's Spark image to avoid manual installation.
  spark-master:
    build:
      context: .
      dockerfile: Dockerfile.spark
    image: ${SPARK_IMAGE}
    container_name: spark-master
    environment:
      # Passamos as credenciais S3A via env vars (vindas do .env)
      S3A_ACCESS_KEY: ${S3A_ACCESS_KEY}
      S3A_SECRET_KEY: ${S3A_SECRET_KEY}
      S3A_ENDPOINT: ${S3A_ENDPOINT}
      AWS_ACCESS_KEY_ID: ${S3A_ACCESS_KEY}
      AWS_SECRET_ACCESS_KEY: ${S3A_SECRET_KEY}
      BRONZE_SESSIONS_DELTA_PATH: ${BRONZE_SESSIONS_DELTA_PATH}
      BRONZE_DRIVERS_DELTA_PATH: ${BRONZE_DRIVERS_DELTA_PATH}
      BRONZE_MEETINGS_DELTA_PATH: ${BRONZE_MEETINGS_DELTA_PATH}
      BRONZE_SESSION_RESULT_DELTA_PATH: ${BRONZE_SESSION_RESULT_DELTA_PATH}
      BRONZE_LAPS_DELTA_PATH: ${BRONZE_LAPS_DELTA_PATH}
      SILVER_MEETINGS_DELTA_PATH: ${SILVER_MEETINGS_DELTA_PATH}
    ports:
      - "7077:7077"   # Spark master port
      - "8080:8080"   # Spark master UI
    volumes:
      - ./spark/jobs:/opt/spark/jobs
      - ./spark/conf:/opt/spark/conf
      - ./spark/lib:/opt/spark/app_lib
      - ./dq:/opt/dq
    command: >
      /bin/bash -lc "
      /opt/spark/sbin/start-master.sh --host spark-master;
      tail -f /opt/spark/logs/*"

  spark-worker:
    build:
      context: .
      dockerfile: Dockerfile.spark
    image: ${SPARK_IMAGE}
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      S3A_ACCESS_KEY: ${S3A_ACCESS_KEY}
      S3A_SECRET_KEY: ${S3A_SECRET_KEY}
      S3A_ENDPOINT: ${S3A_ENDPOINT}
      AWS_ACCESS_KEY_ID: ${S3A_ACCESS_KEY}
      AWS_SECRET_ACCESS_KEY: ${S3A_SECRET_KEY}
      BRONZE_SESSIONS_DELTA_PATH: ${BRONZE_SESSIONS_DELTA_PATH}
      BRONZE_DRIVERS_DELTA_PATH: ${BRONZE_DRIVERS_DELTA_PATH}
      BRONZE_MEETINGS_DELTA_PATH: ${BRONZE_MEETINGS_DELTA_PATH}
      BRONZE_SESSION_RESULT_DELTA_PATH: ${BRONZE_SESSION_RESULT_DELTA_PATH}
      BRONZE_LAPS_DELTA_PATH: ${BRONZE_LAPS_DELTA_PATH}
      SILVER_MEETINGS_DELTA_PATH: ${SILVER_MEETINGS_DELTA_PATH}
    ports:
      - "8081:8081"   # Spark worker UI
    volumes:
      - ./spark/jobs:/opt/spark/jobs
      - ./spark/conf:/opt/spark/conf
      - ./spark/lib:/opt/spark/app_lib
      - ./dq:/opt/dq
    command: >
      /bin/bash -lc "
      /opt/spark/sbin/start-worker.sh spark://spark-master:7077;
      tail -f /opt/spark/logs/*"

# ----------------------------
# Named volumes
# ----------------------------
# A named volume is managed by Docker and persists across restarts.
# Here we persist MinIO data so you don't lose your lakehouse when containers restart.
volumes:
  minio:
