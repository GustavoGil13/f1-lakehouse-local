# Purpose: Local dev docker-compose for the F1 lakehouse (MinIO + Spark cluster).
# Tip: Copy secrets into a .env file at repo root (MINIO_ROOT_USER/MINIO_ROOT_PASSWORD, S3A_*).
# Notes:
# - Run with: docker compose up --build
# - Access MinIO UI: http://localhost:9001  Spark UI: http://localhost:8080 (master), http://localhost:8081 (worker)
# - Persist MinIO data via the named volume `minio`

services:
  # ----------------------------
  # MinIO = "S3-compatible" object storage
  # ----------------------------
  # Local replacement for AWS S3 used by Spark via s3a://
  minio:
    image: minio/minio:latest
    # Run MinIO server and expose console on port 9001
    command: server /data --console-address ":9001"

    # Load admin credentials from .env
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}

    # Persist bucket data on the host
    volumes:
      - minio:/data

    # Host:container port mappings
    ports:
      - "9000:9000"   # S3 API endpoint
      - "9001:9001"   # Web console

  # ----------------------------
  # createbuckets = one-time setup container
  # ----------------------------
  # Uses `mc` (MinIO client) to create the bucket defined in LAKEHOUSE_BUCKET.
  # This container exits after creating the bucket; it is safe to re-run.
  createbuckets:
    image: minio/mc:latest

    # Ensure MinIO is available before running the script
    depends_on:
      - minio

    # The script waits until MinIO is ready, configures an alias, and creates the bucket.
    # `|| true` makes the step idempotent if bucket already exists.
    entrypoint: >
      /bin/sh -c "
      echo 'Waiting for MinIO to be ready...';
      until mc alias set local ${MINIO_ENDPOINT} ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD}; do
        echo 'MinIO not ready yet - sleeping 2s';
        sleep 2;
      done;
      echo 'MinIO is ready. Creating bucket...';
      mc mb -p local/${LAKEHOUSE_BUCKET} || true;
      echo 'Done.';
      "

  mysql:
    image: mysql:5.7
    container_name: mysql
    platform: linux/x86_64
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: metastore
    ports:
      - "3306:3306"

  hive-metastore:
    image: apache/hive:4.0.0
    container_name: hive-metastore
    ports:
      - "9083:9083"
    environment:
      HIVE_METASTORE_DB_HOST: mysql
      HIVE_METASTORE_DB_NAME: metastore
      HIVE_METASTORE_DB_USER: root
      HIVE_METASTORE_DB_PASS: root
      SERVICE_NAME: metastore
    depends_on:
      - mysql
    volumes:
       - ./hive/jars/hadoop-aws-3.3.4.jar:/opt/hive/lib/hadoop-aws-3.3.4.jar
       - ./hive/jars/aws-java-sdk-bundle-1.12.262.jar:/opt/hive/lib/aws-java-sdk-bundle-1.12.262.jar
       - ./hive/core-site.xml:/opt/hive/conf/core-site.xml
    command: >
      bash -lc "
      while ! nc -z mysql 3306; do
        echo 'Waiting for MySQL...';
        sleep 1;
      done;
      /opt/hive/bin/hive --service metastore
      "

  # ----------------------------
  # Spark Master
  # ----------------------------
  # Single-node master used for local experiments. Image is built from Dockerfile.spark.
  spark-master:
    build:
      context: .
      dockerfile: Dockerfile.spark
    image: f1-spark:local
    container_name: spark-master
    environment:
      # Provide S3A credentials + paths to Delta locations via environment (from .env).
      # These are injected into the Spark container so jobs can read/write to MinIO.
      S3A_ACCESS_KEY: ${S3A_ACCESS_KEY}
      S3A_SECRET_KEY: ${S3A_SECRET_KEY}
      S3A_ENDPOINT: ${S3A_ENDPOINT}
      AWS_ACCESS_KEY_ID: ${S3A_ACCESS_KEY}
      AWS_SECRET_ACCESS_KEY: ${S3A_SECRET_KEY}
      HIVE_WAREHOUSE_DIR: ${HIVE_WAREHOUSE_DIR}
      BRONZE_DB: ${BRONZE_DB}
      SILVER_DB: ${SILVER_DB}
      DQ_DB: ${DQ_DB}
    ports:
      - "7077:7077"   # Spark master port
      - "8080:8080"   # Spark master UI
    volumes:
      - ./spark/jobs:/opt/spark/jobs     # place job scripts here
      - ./spark/conf:/opt/spark/conf     # custom Spark configuration (spark-defaults.conf)
      - ./spark/lib:/opt/spark/app_lib   # additional jars/libraries for Spark
    command: >
      /bin/bash -lc "
      /opt/spark/sbin/start-master.sh --host spark-master;
      tail -f /opt/spark/logs/*"

  # ----------------------------
  # Spark Worker
  # ----------------------------
  # Worker node that registers to the master. Keep it lightweight for local testing.
  spark-worker:
    build:
      context: .
      dockerfile: Dockerfile.spark
    image: f1-spark:local
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      # Mirror S3A and path env vars so worker tasks have same access
      S3A_ACCESS_KEY: ${S3A_ACCESS_KEY}
      S3A_SECRET_KEY: ${S3A_SECRET_KEY}
      S3A_ENDPOINT: ${S3A_ENDPOINT}
      AWS_ACCESS_KEY_ID: ${S3A_ACCESS_KEY}
      AWS_SECRET_ACCESS_KEY: ${S3A_SECRET_KEY}
      HIVE_WAREHOUSE_DIR: ${HIVE_WAREHOUSE_DIR}
      BRONZE_DB: ${BRONZE_DB}
      SILVER_DB: ${SILVER_DB}
      DQ_DB: ${DQ_DB}
    ports:
      - "8081:8081"   # Spark worker web UI
    volumes:
      - ./spark/jobs:/opt/spark/jobs
      - ./spark/conf:/opt/spark/conf
      - ./spark/lib:/opt/spark/app_lib
    command: >
      /bin/bash -lc "
      /opt/spark/sbin/start-worker.sh spark://spark-master:7077;
      tail -f /opt/spark/logs/*"


volumes:
  minio:
